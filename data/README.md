# Files
* `scfg_generator.py`: Generates a [parallel corpus](#parallel-corpus) based on the words defined in the script. Each line of the output contains a sentence in the source language and its translation in the target language, separated by a single tab.
* `build_dataset.py`: Takes the [parallel corpus](#parallel-corpus) generated by `scfg_generator.py`, randomizes the order of the sentences and splits them into a [training (80%)](#training-set), [validation (10%)](#validation-set) and [test set (10%)](#test-set).

# Directories
The three directories (`mix`, `vso` and `vos`) contain the files generated by `scfg_generator.py` and `build_dataset.py`. Additional files that are generated while pre-processing the data, training the model or translating data also get added to these directories.

## Parallel corpus
* `par_corp.txt`: all sentences in the source and target language

## Training set
* `src_train.txt`: sentences of the training set in the source language
* `tgt_train.txt`: sentences of the training set in the target language

## Validation set
* `src_val.txt`: sentences of the validation set in the source language
* `tgt_val.txt`: sentences of the validation set in the target language

## Test set
* `src_test.txt`: sentences of the test set in the source language
* `tgt_test.txt`: sentences of the test set in the target language

## Files generated after pre-processing data
* `prepared_data.vocab.pt`: serialized PyTorch file containing training data
* `prepared_data.valid.0.pt`: serialized PyTorch file containing validation data
* `prepared_data.train.0.pt`: serialized PyTorch file containing vocabulary data

## Files generated after training a model
* `trained_model_step_N.pt`: the trained model, where N is the number of steps (a checkpoint is saved after every 5000 steps)

## Files generated after translating data
* `out_test.txt`: sentences as translated by the model